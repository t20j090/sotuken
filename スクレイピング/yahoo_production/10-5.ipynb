{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79a65d1-266b-448e-a96f-afc63185b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "#ヤフーニュースの情報を取得\n",
    "def yahoo_news_scraping(topic):\n",
    "    url = 'https://news.yahoo.co.jp/'\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    \n",
    "    list_item = soup.find_all('li')\n",
    "    #もしかしたらエラーの原因になるかも\n",
    "    list_item = list_item[12:21]\n",
    "    \n",
    "    # <a>要素からトピックを取得\n",
    "    a_contents = [li.a for li in list_item]\n",
    "    \n",
    "    articles_dict = []\n",
    "    topic = topic \n",
    "    #トピックのURL取得\n",
    "    for i, a_content in enumerate(a_contents):\n",
    "        if topic in a_content.text:\n",
    "            categories_num = i\n",
    "    \n",
    "    a_content = a_contents[categories_num]\n",
    "    \n",
    "    topic_url = a_content['href']\n",
    "    URL = url + topic_url\n",
    "    \n",
    "    r = requests.get(URL)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    #クラス名が変わるので注意！！\n",
    "    elements = soup.select('.sc-fXazdy.UjHkE')\n",
    "    \n",
    "    elements = elements[:5]\n",
    "    \n",
    "    news_urls = []\n",
    "    news_titles = []\n",
    "    #ニュースのURLとタイトルを取得\n",
    "    for element in elements:\n",
    "        url = element.a['href']\n",
    "        title = element.text[1:]\n",
    "        news_urls.append(url)\n",
    "        news_titles.append(title)\n",
    "            \n",
    "    news_text = []\n",
    "    #ニュースの本文を取得\n",
    "    for news_url in news_urls:\n",
    "        r = requests.get(news_url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        #クラス名が変わるので注意！！ \n",
    "        elements = soup.select('.sc-iMCRTP.ePfheF.yjSlinkDirectlink.highLightSearchTarget')\n",
    "    \n",
    "        text = \"\"\n",
    "    \n",
    "        #ニュース記事の本文が配列で区切られていた場合\n",
    "        for i in range(0, 8):\n",
    "            try:\n",
    "                if (i == 0):\n",
    "                    text = elements[0].text\n",
    "                else:\n",
    "                    text += elements[i].text\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        #ページが1つだけのとき、エラーが出るため\n",
    "        try:\n",
    "            #クラス名が変わるので注意！！ （取得例：２ページ)\n",
    "            page_element = soup.select('.sc-cpUASM.bqNXll')\n",
    "            page = page_element[0].text         \n",
    "            page_str = ''.join(filter(str.isdigit, page))\n",
    "            page_num = int(page_str)\n",
    "        except:\n",
    "            page_num = 1\n",
    "            \n",
    "        #ページ数が複数のとき、ページ遷移し本文を取得\n",
    "        if (page_num > 1):\n",
    "            for i in range(2, page_num + 1):\n",
    "                news_url_page = news_url + \"?page=\" + str(page_num)\n",
    "                r = requests.get(news_url_page)\n",
    "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "                #クラス名が変わるので注意！！ \n",
    "                elements = soup.select('.sc-iMCRTP.ePfheF.yjSlinkDirectlink.highLightSearchTargett')\n",
    "                #ニュース記事の本文が配列で区切られていた場合\n",
    "                for i in range(0, 8):\n",
    "                    try:\n",
    "                        text += elements[i].text\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "        news_text.append(text)\n",
    "    \n",
    "        time.sleep(0.5)\n",
    "\n",
    "    articles = []\n",
    "    for url, title, text in zip(news_urls, news_titles, news_text):\n",
    "        article = {\n",
    "            \"title\" : title,\n",
    "            \"URL\" : url,\n",
    "            \"text\" : text\n",
    "        }\n",
    "        articles.append(article)\n",
    "\n",
    "    return articles\n",
    "\n",
    "#CNNニュースの情報を取得\n",
    "def cnn_news_scraping(topic):\n",
    "    url = 'https://www.cnn.co.jp'\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    \n",
    "    #クラス名が変わるので注意！！\n",
    "    elements = soup.select('.pg-container li a')\n",
    "    elements = elements[6:11] + [elements[14], elements[16]]\n",
    "    \n",
    "    topic = topic\n",
    "    #トピックのURL取得\n",
    "    for i, element in enumerate(elements):\n",
    "        if topic in element.text:\n",
    "            categories_num = i\n",
    "    \n",
    "    element = elements[categories_num]\n",
    "    \n",
    "    topic_url = element['href']\n",
    "    URL = url + topic_url\n",
    "    \n",
    "    r = requests.get(URL)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    \n",
    "    #クラス名が変わるので注意！！\n",
    "    elements = soup.select('.cd a')\n",
    "    \n",
    "    elements = elements[:3]\n",
    "    \n",
    "    news_urls = []\n",
    "    news_titles = []\n",
    "    #ニュースのURLとタイトルを取得\n",
    "    for element in elements:\n",
    "        news_url =  url + element['href']\n",
    "        news_title = element.text\n",
    "    \n",
    "        news_urls.append(news_url)\n",
    "        news_titles.append(news_title)\n",
    "    \n",
    "    news_text = []\n",
    "    #ニュースの本文を取得\n",
    "    for i, news_url in enumerate(news_urls):\n",
    "        text = \"\"\n",
    "        \n",
    "        for j in range(1, 7):\n",
    "            num = str(j)\n",
    "            page = '-' + num + '.html'    \n",
    "            # 文字列の置換を行い新しいURLを生成\n",
    "            news_url_page = news_urls[i].replace('.html', page)\n",
    "    \n",
    "            #ページ数が複数のとき、ページ遷移し本文を取得\n",
    "            response = requests.get(news_url_page)\n",
    "    \n",
    "            # レスポンスのステータスコードを確認\n",
    "            if response.status_code != 404: \n",
    "                r = requests.get(news_url_page)\n",
    "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "                #クラス名が変わるので注意！！ \n",
    "                elements = soup.find(id = 'leaf-body')\n",
    "                text += elements.text\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "        news_text.append(text)\n",
    "\n",
    "    articles = []\n",
    "    for url, title, text in zip(news_urls, news_titles, news_text):\n",
    "        article = {\n",
    "            \"title\" : title,\n",
    "            \"URL\" : url,\n",
    "            \"text\" : text\n",
    "        }\n",
    "        articles.append(article)\n",
    "\n",
    "    return articles\n",
    "\n",
    "#データを整える\n",
    "def clean_data(articles):\n",
    "    for i in range(len(articles)):\n",
    "        articles[i]['title'] = articles[i]['title'].replace('\\u3000', '') \n",
    "        articles[i]['title'] = articles[i]['title'].lstrip('0123456789')\n",
    "        articles[i]['text'] = articles[i]['text'].replace('\\u3000', '')\n",
    "        articles[i]['text'] = articles[i]['text'].replace('\\n', '')\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5cde1a9-abaf-4051-99f2-c85ad636de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dict = yahoo_news_scraping('経済')\n",
    "articles = clean_data(articles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78beeccd-381f-489a-9303-1b6ce50e5b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '「雨の日に洗車」なぜ勧める？ ガソリンスタンドで「推奨」するワケ… 実は「雨で汚れ」は落ちない？くるまのニュース10/4(水)14:10',\n",
       "  'URL': 'https://news.yahoo.co.jp/articles/2e0cb7020e99449874ea81b6298aa3567dbdb13c',\n",
       "  'text': '雨が降ってくると「クルマが汚れているからちょうどいい」と言う人がいます。一方でガソリンスタンドなどでは「雨の日洗車キャンペーン」や「雨の日は洗車がオトク！」といった宣伝をしていますが、なぜ雨の日に洗車を勧めてくるのでしょうか。【衝撃画像！】「えっ…！」これが危険すぎる「タイヤの末路」です（17枚）クルマの洗車は、こだわる人であれば自らの手で洗車することもあり、様々な洗車グッズも展開されています。一方で、自分でやるのが面倒と考える人もおり、その場合は汚れが目立った際に洗車機で洗うという人も一定数存在します。どちらの場合でも、雨の日にであれば「雨水が汚れを洗い流してくれるので洗車は必要ない」と考える人もいるかもしれません。クルマは新車時であれば、塗装やワックスなどがボディを守ってくれています。しかし時間が経つにつれ、ボディを守る力が弱まり、ホコリやゴミが付着しやすくなってしまいます。そして、そのホコリやゴミなどが酸化することで頑固な汚れの原因となるのです。また、小さな虫の死骸もボディにとって害となり、虫の死骸にはタンパク質やリン酸が含まれており、塗装面に悪影響を及ぼし「酸化クレーター」と呼ばれる凸凹が生じることも。さらに雨には大気中のチリやゴミといった汚れが含まれており、そのまま放置した場合、水垢やウォータースポットになります。ほかにも、ボディに付着した砂や鉄は、雨に濡れると泥団子状の塊となり、濡れている間は柔らかいものの、雨がやんで乾くと硬くなり、ボディを傷付ける原因となるようです。こうしたことをふまえてクルマの洗車について洗車専門店では次のように話しています。「基本的には、1、2か月程度の頻度で洗車をおこなっていただくことをおすすめしています。また雨にあたることでクルマのボディに傷がつきやすい状態になってしまいます。そのため『雨で汚れが落ちるだろう』という考えから洗車を怠っていた場合、塗装や誇りなどのごみの付着など、ボディの状態の悪化に繋がるおそれがあります」では、なぜガソリンスタンドなどでは雨の日に洗車を勧めるのでしょうか。首都圏のガソリンスタンドのスタッフは次のように話します。「雨の日に洗車をお勧めする理由には、スタンド側の意図が大きいです。単純に晴天時よりも『汚れを落としやすい』『拭き上げ／吹き残しの心配がいらない』という作業面の効率により『洗車時間を短縮出来る』ということが挙げられます。さらに、店舗としては雨天時の利用を促進するひとつとして、『雨の日洗車回数券』など雨の日限定でリピートして頂けるようなサービスも存在します」※※※雨の日に洗車をするということは、「雨でクルマの汚れを落としてくれるだろう」というよりも、「雨で新たに汚れたり、さらに頑固な汚れになるのを防ぐ」というものと、ガソリンスタンド側の営業戦略という両方の考えが存在していると言えます。'},\n",
       " {'title': '「生活保護」を受けると近所にバレますか？ 病気で働けず、生活保護を受けたいと考えています…ファイナンシャルフィールド10/5(木)11:31',\n",
       "  'URL': 'https://news.yahoo.co.jp/articles/c34f0c6666b72e61a6412b103a976aa1cbd77ffe',\n",
       "  'text': '病気で働けず、生活保護を受けたいと考えている人もいるでしょう。病気に限らず、ほかのさまざまな理由で働けなくなった人にとって、生活保護は最後の砦（とりで）といえます。しかし、受給しているのを近所に知られるのが嫌だと感じ、申請をためらっている人もいるのではないでしょうか。 そこで今回は、生活保護を受給できる人の条件の紹介とともに、受けていることが近所に知られやすい状況を解説していきます。生活保護制度とは、資産や能力などをすべて活用しても生活に困窮する人に対し、その困窮の程度に応じた保護を行い、健康で文化的な最低限の生活を保障し、その自立を助長するための制度です。 支給される保護費は、地域や世帯状況によって違ってきます。生活保護制度で、保護費を受給できるのは「世帯収入が、居住地の最低生活費よりも低い人」「病気や障害のために働けない人」「年金制度や国の融資制度などが利用できない人」などです。 世帯収入は家族全員分であり、具体的には「労働による賃金・親族からの仕送り・退職金・年金・失業保険や傷病手当金などの公的手当」が当てはまります。また、生命保険などの保険金や車や家などの売却金も、世帯収入の一部です。ちなみに、最低生活費とは、健康で文化的な最低限の生活を送るために必要な費用をいいます。これは、日本国憲法第25条の規定によるものです。 病気や障害のために働けない人は、申請の際には診断書や障害者手帳を持っていくことが推奨されます。これにより、通りにくいとされている生活保護の申請が比較的通りやすくなるでしょう。年金制度や国の融資制度などを生活保護よりも優先的に利用することは、生活保護法によって規定されています。そのため、年金制度や国の融資制度などを利用できない点が、生活保護を受給できる条件の1つとなるのです。生活保護を受けていることは、担当しているケースワーカーが口に出さない限りバレる心配はありません。ただし、受給する際の扶養照会により、民法上で扶養義務のある三等親までの親族には知られてしまいます。これは、扶養義務者による扶養が、生活保護法による保護に優先しておこなわれることが理由です。 理由によっては扶養照会が行われない場合もありますが、原則として自治体には生活保護の申請者の親族に金銭的援助が可能かどうかを確認する義務が発生します。この場合以外、生活保護を受けているとバレることは、基本的にありません。 しかし、それでも近所にバレてしまう可能性の高い状況が2つあります。 1つは、ケースワーカーの訪問を知られてしまったときです。受給者の自宅には、生活の様子を確認するために年に数回ケースワーカーが訪問します。ケースワーカーの来る理由を知っている人にこの様子を見られると、受給がバレる可能性があります。対策として、ケースワーカーに人目の少ない時間に来てもらうようにしましょう。例えば、できるだけ午前中の早い時間などです。 もう1つの状況として、病院の窓口の受付担当が知り合いなどだった場合に、受給がバレる可能性があります。受給者は国民健康保険などに加入できないため、病院での診察には医療証の提示が必要です。医療証は生活保護の受給者が使うものであるため、それを見た受付担当には受給していることが知られてしまいます。対策としては、自宅から遠い病院を利用するか、受付業務が機械化されているような大きな病院を利用することです。 ほかには、受給者に学校に通う子どもがいる場合に注意しましょう。子どもが修学旅行に参加する際、基本的に保険証のコピーが必要です。受給者の場合は、当然医療証をコピーします。これをうっかり誰かに見られると、受給がバレる可能性があるでしょう。生活保護は権利ですが、自身の事情はあまり知られたくないという人もいるでしょう。そのような場合は、ケースワーカーの訪問や医療証の提示などの際に注意しましょう。 とはいえ、扶養照会をする親族以外であれば、基本的に生活保護を受けていると知られることはありません。もしも生活に困っているのであれば、そういった不安点も含めて一度相談してみると良いでしょう。 出典厚生労働省 生活保護制度 執筆者：FINANCIAL FIELD編集部ファイナンシャルプランナー'},\n",
       " {'title': '新卒がいつも定時3分前に退社…「PCの誤作動です」と言い訳、どう対応すればよい？ファイナンシャルフィールド10/5(木)11:42',\n",
       "  'URL': 'https://news.yahoo.co.jp/articles/32586cd8720d8d781acd43fbbfd501108cb30979',\n",
       "  'text': '社会人として働くなかで、定時退社するためにパソコンの前で打刻待機をする方や、いつも定時前に退勤してしまい、かつそれをパソコンの誤作動と言い訳する人に出会ったことはありませんか。 まれに見るこうした人材に対して、どのように注意・指導すればよいのか頭を悩ませている方もいらっしゃるでしょう。今回は、定時前の退社を繰り返す人の給料について、その対応の仕方をご紹介します。賃金は一般的に、定められた時間、きちんと労働したことを前提に計算・支給されます。 そのため定時前に退社すれば、その分、給料が差し引かれる可能性が高いといえるでしょう。とはいえ給料体系は企業や雇用契約ごとで異なるため、正確な計算方法は就業規則を確認する必要があります。 ただし、定時前に退社しても給料がカットされないケースもあります。会社都合、つまり上司から「もう今日は仕事がないので帰宅してよい」と言われた場合です。雇用主からこのように言われた場合は、定時前に退社しても給料がカットされることはありません。今回のように定時前に退社していることを指摘しても「パソコンの誤作動」と主張された場合は、まず「本当に定時前に無断で退社しているのか」を確認する必要があります。 事実だと確認が取れた場合は、できるだけ早いタイミングで注意・指導をするとともに、定時前に退社すると給料がカットされることをあらためて説明する必要があるでしょう。注意するタイミングが遅くなれば「今までなんだかんだ許されていたのに、突然なぜ」と、新たなトラブルに発展してしまうかもしれません。 新卒とはいえ、「少しくらい定時前に退社しても問題ない」という認識を持つことは許されません。毎日定時3分前に退社してしまう場合は、なぜそうしてしまうのか事情を聴く必要があるでしょう。例えば、子どもがいて、定時で退社をしていたのでは保育園のお迎えに間に合わない。あるいは、家族がけがをしてしまい、一時的に数分だけ早く退社したかったなどの事情があるのかもしれません。こうした場合、企業によっては、一時的に柔軟な対応を取ることで解決できる可能性があります。 とはいえ、どのような事情があっても、無断で就業規則を破る行為そのものは問題です。あらためて就業規則を説明したり解決を試みたりしたうえで、それでも定時前退社がやまないようなら、雇用契約や勤務時間の見直しが必要になるでしょう。定時前の退社を繰り返す人がいる場合は、まずは状況把握とヒアリングから始めるのがよいでしょう。そのうえで、話し合いをしたのち適切な働き方を模索する必要もあるかもしれません。 一方的な注意にとどめず、まずは話を聞くことから始めましょう。 執筆者：FINANCIAL FIELD編集部ファイナンシャルプランナー'},\n",
       " {'title': 'ウクライナ軍「奪った戦車が故障したんだが」ロシアの\"お客様相談室\"にクレーム!? 戦闘で鹵獲のT-72主力車乗りものニュース10/5(木)11:42',\n",
       "  'URL': 'https://news.yahoo.co.jp/articles/919a0161c6e830135be004cefb854ecf590482cc',\n",
       "  'text': 'ウクライナ軍の戦車兵が鹵獲したT-72戦車が故障したため、製造元であるロシアのウラルヴァゴンザヴォートのサポートセンターに連絡したことが話題となっています。【画像】えっ…！これが「ロシア製戦車」を堂々と使うウクライナ軍です大胆にもウラルヴァゴンザヴォートのサポートセンターに電話をかけたのは、コールサインを「コチェブニク」と名乗るウクライナ陸軍の戦車兵です。サポートセンターに電話がつながると、ロシア語で「オイル漏れを起こしている、エンジンの調子が良くない。サスペンションもダメでコンプレッサーが故障している。砲塔の油圧も不具合があり手動で動かさなければならない」と苦情を伝えました。コチェブニクのロシア語は適切だったのか、全く気づかれておらず、サポートセンターは設計局に問題を提起すると約束しました。合間にはさらっと「まあ、ソビエト製の古い戦車よりはクソほど良くなった」と皮肉たっぷりにコメントする場面も。その後、コチェブニクはウラルヴァゴンザヴォートの工場所長にもにも直接連絡。そこでもバレることはなく、メッセンジャーアプリの「WhatsApp（ワッツアップ）」を使用して戦車の問題を詳しく説明して欲しいと求められました。その後も全くバレることはなく、コチェブニクは自身の身分を明かし「ありがとう、気をつけてください。ウクライナに栄光あれ」と言って電話を切りました。なお、ロシア軍の戦車をウクライナ軍は少なくとも440両は鹵獲していると、2022年10月の段階でイギリス国防省が分析しており、現在は500両程度の戦車をウクライナ軍が手にしたと見られています。コチェブニクによると、故障した鹵獲車両はT-72B3というタイプで、T-72の中でも2012年製という比較的新しい車両です。ウクライナが保有するT-72とは互換性がない部分もあるようですが、本当にウクライナ側で修理できないレベルではないようです。'},\n",
       " {'title': '大阪IR契約締結も、「夢洲アクセス路線」問題がさっぱり解決していない渋い現実Merkmal10/5(木)11:51',\n",
       "  'URL': 'https://news.yahoo.co.jp/articles/7a4cf0d7291f9134b104c74bcf802cad07542258',\n",
       "  'text': '大阪市此花区の人工島・夢洲（ゆめしま）で整備予定の統合型リゾート施設（IR）が本契約に至った。開業時期の先送りが続き、夢洲アクセス路線の計画に影響を与えてきたが、まだ課題が残る。【画像】えっ…！ これが60年前の「新大阪駅」です（計11枚）大阪市住之江区の人工島・咲洲（さきしま）にある海沿いの遊歩道から北側を眺めると、夢洲が見える。大阪・関西万博が2025年に開催され、IRが2030年秋ごろの開業を目指す場所だ。咲洲と夢洲の直線距離は1km少し。IRや万博予定地は見えないが、コンテナターミナルの照明塔がくっきり浮かぶ。咲洲と夢洲間には海底部延長約800mの夢咲トンネルが2009（平成21）年に開通、道路だけが供用されている。2024年度には鉄道も開通し、鉄路が咲洲にある大阪メトロ中央線のコスモスクエア駅から夢洲中央部に新設される夢洲駅まで約3km延びる。路線名は北港テクノポート線。夢洲と大阪市中心部を結ぶ南ルートで、万博会場へのメインアクセスとなる。工事は夢洲駅側から2本の単線トンネルが掘り進められ、5月に夢咲トンネルに到達した。夢洲駅の建設工事も並行して続いており、大阪港湾局は「2024年度中の開業に向けて順調に進んでいる」と胸を張る。運行する大阪メトロは万博に向けて導入した新型車両400系の営業運転を始めた。宇宙船を意識した斬新なデザインで、未来社会の姿を示す万博にふさわしい車両にしたという。さらに、延伸区間の第二種鉄道事業許可を国土交通省へ申請している。大阪メトロは「万博を見据え、準備が整ってきた。あとは許可だけ」と力を込めた。中央線に乗り入れる近畿日本鉄道は、IR開業時点で奈良方面から直通列車を走らせる計画。だが、大阪市と奈良県奈良市を結ぶ近鉄奈良線は奈良県生駒市の生駒駅で、相互乗り入れする中央線、近鉄けいはんな線と接続しているものの、奈良線と中央線、けいはんな線の集電方式が異なり、これまで直通できなかった。そこで、近鉄は両方式に対応できる集電装置の試作品を開発し、各種試験に入っている。近鉄は「夢洲エリアを訪れる海外からのIR来場者らを近鉄沿線の奈良や伊勢方面に引き込みたい」と意欲的だ。'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde1260-1e22-49cf-aeac-405fa05d0fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
