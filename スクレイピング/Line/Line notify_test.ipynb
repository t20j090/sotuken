{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1204d3dc-627b-48e0-89c9-ce80bd83a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a21dd215-2d94-449f-8d94-df16c2fe70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for article_url in article_urls:\n",
    "        send_line_notify('ニュース', article_url)\n",
    "\n",
    "def send_line_notify(notification_message, url):\n",
    "    \"\"\"\n",
    "    Lineに通知する\n",
    "    \"\"\"\n",
    "    line_notify_token = 'LMr5ZeQuAsAz6GMxaMJyzLuhGBO3VvVIFxTwfSLUvz0'\n",
    "    line_notify_api = 'https://notify-api.line.me/api/notify'\n",
    "    headers = {'Authorization': f'Bearer {line_notify_token}'}\n",
    "    data = {'message': url}\n",
    "    requests.post(line_notify_api, headers = headers, data = data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ad0c55-d3cc-4e0a-bb1e-be6dfbd9f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def scraping_news():\n",
    "    url = 'https://news.yahoo.co.jp/'\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    elements = soup.select('.sc-fKgJPI.fFnFHR')\n",
    "    \n",
    "    topics_list = []\n",
    "    for element in elements:\n",
    "        url_href = element.a['href']\n",
    "        topics_list.append(url_href)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    topics_list.pop(-1)\n",
    "    \n",
    "    topics_urls_list = []\n",
    "    for topic in topics_list:\n",
    "        topics_urls_list.append(url + topic)\n",
    "    \n",
    "    articles_dict = []\n",
    "    \n",
    "    for url in topics_urls_list:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        \n",
    "        elements = soup.select('.sc-fXazdy.UjHkE')\n",
    "        elements = elements[:5]\n",
    "    \n",
    "        news_urls = []\n",
    "        news_titles = []\n",
    "        for element in elements:\n",
    "            url = element.a['href']\n",
    "            title = element.text\n",
    "\n",
    "            news_urls.append(url)\n",
    "            news_titles.append(title)\n",
    "            \n",
    "            time.sleep(1)\n",
    "    \n",
    "        news_text = []\n",
    "        for news_url in news_urls:\n",
    "            r = requests.get(news_url)\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            \n",
    "            elements = soup.select('.sc-dYXZXt.jzZUwL.yjSlinkDirectlink.highLightSearchTarget')\n",
    "            text = elements[0].text\n",
    "            \n",
    "            news_text.append(text)\n",
    "    \n",
    "            time.sleep(1)\n",
    "            \n",
    "        articles = []\n",
    "        for url, title, text in zip(news_urls, news_titles, news_text):\n",
    "            article = {\n",
    "                \"title\" : title,\n",
    "                \"URL\" : url,\n",
    "                \"text\" : text\n",
    "            }\n",
    "            articles.append(article)\n",
    "        articles_dict.append(articles)\n",
    "\n",
    "    return articles_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acab610c-5ed7-4485-9c09-2128a4a106f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dict = scraping_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14a26bf1-9d05-48a3-9bd4-7be5f30dbeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(articles_dict)):\n",
    "    for j in range(len(articles_dict[0])):\n",
    "        articles_dict[i][j]['title'] = articles_dict[i][j]['title'].replace('\\u3000', '') \n",
    "        articles_dict[i][j]['title'] = articles_dict[i][j]['title'].lstrip('0123456789')\n",
    "        articles_dict[i][j]['text'] = articles_dict[i][j]['text'].replace('\\u3000', '')\n",
    "        articles_dict[i][j]['text'] = articles_dict[i][j]['text'].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71000843-730c-4db5-bb2f-721409d23f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dict = articles_dict[0]\n",
    "article_urls = []\n",
    "for i in article_dict:\n",
    "    article_urls.append(i['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87474d97-65d3-4f45-9ffe-be9ac0602547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://news.yahoo.co.jp/articles/3645384478280614935345122a6ef4ce1a18b2c0',\n",
       " 'https://news.yahoo.co.jp/articles/158fc26d76f503af49a473ec697b956565b16d23',\n",
       " 'https://news.yahoo.co.jp/articles/18ba9b1abcce1cbd8c8b13e957fa1b0992f4c071',\n",
       " 'https://news.yahoo.co.jp/articles/39d352b10cb8c2ab6a1bccaa2f579aecf7db1a26',\n",
       " 'https://news.yahoo.co.jp/articles/22130ac69e6cd77092e32bf370904802eccd3e92']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d1ad6-5886-49f4-83dd-cfeee7f82502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
